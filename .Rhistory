_HiStOrY_V2_
1- pnorm(2, 0, 1)
1- pnorm(2, 0, 4)
0.8469-0.6099
exp(-5)
exp(-4)
exp(-4) *4
exp(-4) *8
0.07326+0.1465
dpois(1, 4)
dpois(1, 4) + dpois(2, 4)
1 - ppois(4, 4)
#Q4.5-(d)
ppois(2, 6)
#Q4.5-(d)
ppois(2, 6)
#Q4.5-(b)
dpois(1, 6/7)
#Q4.5-(c)
1 - ppois(1, 6/7)
#Q4.5-(c)
1 - dpois(0, 6/7)
#Q4.5-(c)
1 - ppois(1, 6/7)
# Subset the data
subset_data <- cvbase[(cvbase$employstatus_1 == 1 | cvbase$employstatus_2 == 1|
cvbase$employstatus_3 == 1) | cvbase$employstatus_9 ==1, ]
rm(list = ls())
setwd("/Users/siddharthjadli/Monash/2024Sem1/FIT3152/Ass01")
# Load necessary packages
library(reshape2)  # For melt()
library(ggplot2)   # For data visualization
library(tidyr)     # For gather()
library(dplyr)
library(patchwork) # To combine two plots
library(MASS)      # Load the MASS package for stepwise regression
library(cluster)   # For clustering
set.seed(32014252) # my student ID
cvbase = read.csv("PsyCoronaBaselineExtract.csv")
cvbase = read.csv("PsyCoronaBaselineExtract.csv")
cvbase <- cvbase[sample(nrow(cvbase), 40000), ] # 40000 rows
set.seed(32014252) # my student ID
cvbase = read.csv("PsyCoronaBaselineExtract.csv")
# 1. Descriptive analysis and pre-processing (6 Marks)
## (a) Describe the data overall
### Initial View of the dataset
cvbase <- cvbase[sample(nrow(cvbase), 40000), ] # 40000 rows
```{r}
# View data structure
dim(cvbase)
str(cvbase)
# Summary statistics for numerical variables
summary(cvbase)
# Missing values
colSums(is.na(cvbase))
# Missing values
colSums(is.na(cvbase))
# Get row indices of empty_records
empty_row_indices <- which(rowSums(is.na(cvbase[, paste0("employstatus_", 1:10)])) == 10 | rowSums(is.na(cvbase[, paste0("coronaClose_", 1:6)])) == 6)
length(empty_row_indices)
# Remove rows with all NA
cvbase <- cvbase[-empty_row_indices, ]
# Iterate through each column and replace NA values with 0
for (col in paste0("employstatus_", 1:10)) {
cvbase[[col]][is.na(cvbase[[col]])] <- 0
}
for (col in paste0("coronaClose_", 1:6)) {
cvbase[[col]][is.na(cvbase[[col]])] <- 0
}
for (col in paste0("coronaClose_", 1:6)) {
cvbase[[col]][is.na(cvbase[[col]])] <- 0
}
# Find rows where all rankOrdLife variables are NA
rows_with_all_na <- which(rowSums(is.na(cvbase[, paste0("rankOrdLife_", 1:6)])) == 6)
length(rows_with_all_na)
# Remove rows with all NA for rankOrdLife variables
cvbase <- cvbase[-rows_with_all_na, ]
# Find rows where at least one variable is NA
rows_with_na <- which(!complete.cases(cvbase))
length(rows_with_na)
dim(cvbase)
# Subset the dataframe based on these rows
cvbase <- cvbase[-rows_with_na, ]
dim(cvbase)
colSums(is.na(cvbase))
# Calculate the frequency of ones in each employstatus variable
employstatus_frequency <- sapply(1:10, function(i) sum(cvbase[[paste0("employstatus_", i)]] == 1))
# Convert the frequency calculation into a data frame
employstatus_data <- data.frame(
emp_status = c("Employed (1-24 hpw)", "Employed (24-39 hpw)", "Employed (40+ hpw)", "Unmployed (looking)", "Unemployed (not looking)", "Homemaker", "Retired", "Disabled", "Student", "Volunteering"),
frequency = employstatus_frequency)
# Reorder the rows of employstatus_data based on the frequency column
employstatus_data <- employstatus_data[order(employstatus_data$frequency, decreasing = TRUE), ]
# Create a bar plot using ggplot with reordered categories
ggplot(employstatus_data, aes(x = factor(emp_status, levels = employstatus_data$emp_status), y = frequency)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Distribution of Employment Status", x = "", y = "Frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels by 45 degrees
# Subset the data
subset_data <- cvbase[(cvbase$employstatus_1 == 1 | cvbase$employstatus_2 == 1|
cvbase$employstatus_3 == 1) | cvbase$employstatus_9 ==1, ]
# Calculate the percentage distribution of isolation variables
isolation_vars <- c("isoFriends_inPerson", "isoOthPpl_inPerson",
"isoFriends_online", "isoOthPpl_online")
isolation_dist <- sapply(subset_data[isolation_vars], function(x) prop.table(table(x)) * 100)
# Create a pie chart with color-coded segments and a legend
par(mfrow=c(2, 2))  # Set the layout for the pie charts
for (i in 1:4) {
colors <- rainbow(length(isolation_dist[, i]))  # Generate colors for segments
pie(isolation_dist[, i],
col = colors, main = paste("Distribution of", colnames(isolation_dist)[i]))
}
# Set working directory to desktop
setwd("/Users/Sid/Desktop/Monash/FIT3152/Ass03")
pwd
dir
list(dir)
list(dir())
# Set working directory to desktop
setwd("/Users/Sid/Desktop/Monash/FIT3152/Ass03")
# Set working directory to desktop
setwd("/Users/Sid/Desktop/Monash/FIT3152/Ass03")
# Cleaning up the environment before starting
rm(list = ls())
set.seed(32014252) # Setting seed for reproducability
# LIBRARIES
library(tm)
library(slam)       # Matrices and arrays
library(SnowballC)  # Stemming
library(proxy)      # Cosine distances
library(igraph)     # Graph analysis
library(igraphdata) # Graph analysis
# Get file path to folder "test" where the documents are located
cname = file.path(".", "Corpus")
docs = Corpus(DirSource((cname)))
print(summary(docs))
# Set working directory to desktop
setwd("/Users/Sid/Desktop/Monash/FIT3152/Ass03")
# Cleaning up the environment before starting
rm(list = ls())
set.seed(32014252) # Setting seed for reproducability
# LIBRARIES
library(tm)
library(slam)       # Matrices and arrays
library(SnowballC)  # Stemming
library(proxy)      # Cosine distances
library(igraph)     # Graph analysis
library(igraphdata) # Graph analysis
# Get file path to folder "test" where the documents are located
cname = file.path(".", "Corpus")
docs = Corpus(DirSource((cname)))
print(summary(docs))
rm(list = ls())
setwd("/Users/Sid/Desktop/Monash/FIT3152/Ass03")
# Set working directory to desktop
setwd("/Users/siddharthjadli/Study/Monash/2024Sem1/FIT3152/Ass03")
# Cleaning up the environment before starting
rm(list = ls())
set.seed(32014252) # Setting seed for reproducability
# LIBRARIES
library(tm)
library(slam)       # Matrices and arrays
library(SnowballC)  # Stemming
library(proxy)      # Cosine distances
library(igraph)     # Graph analysis
library(igraphdata) # Graph analysis
# Get file path to folder "test" where the documents are located
cname = file.path(".", "Corpus")
docs = Corpus(DirSource((cname)))
print(summary(docs))
# Tokenise
# Hyphen to space, ref Williams
toSpace <- content_transformer(function(x, pattern)
gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
# Filter Words by removing stop words and white space
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
# Stem
docs <- tm_map(docs, stemDocument, language = "english")
# Create document term matrix
dtm <- DocumentTermMatrix(docs)
# Remove sparse terms
dtms <- removeSparseTerms(dtm, 0.4)  # Remove columns with 40% empty (0) cells
dtms = as.matrix(dtms)
write.csv(dtms, "dtms.csv")
# Clustering
set.seed(32014252) # keep clustering consistent
distmatrix = proxy::dist(dtms, method = "cosine")
fit = hclust(distmatrix, method = "ward.D")
plot(fit, xlab = "")
